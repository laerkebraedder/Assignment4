---
title: "The Alien Game"
author: "Lærke Brædder"
date: "2023-05-02"
output: html_document
---

```{r}
pacman::p_load(tidyverse,
               future,
               furrr,
               patchwork,
               cmdstanr,
               brms)
```


Implementing a GCM to the alien categorization game.

## The generalized context model (GCM)
In this setup, which is inspired by Kruschke 1993, participants go through 8 blocks of trials. Within each trial block, the participants view 8 stimuli, which are the same across every block. It is then the participant's job to categorize/classify the stimuli as belonging to either category A or category B. After categorizing a stimulus, the participant is given feedback about which category the stimulus truly belongs to. The stimuli which are shown to the participant's vary along two continuous dimensions (in Kruschke those are the height of a square and the position of a line with it).


Following on the structure in Kruschke 1993, we need to start simulating data. Let’s start with the stimuli.


In session 1, danger depends on eyes and spots
```{r}
# Defining the stimuli, their height and position features, and their category
stimulus <- seq(32)
eyes <- c(0,1)
spots <- c(0,1)
color <- c(0,1)
teeth <- c(0,1)
arms <- c(0,1)
category <- c(0,1)

experiment <- expand.grid(eyes=eyes, spots=spots, color=color, teeth=teeth, arms=arms)
experiment$stimulus <- seq(32)

# Creating the conditional danger column
experiment$dangerous <- ifelse(experiment$eyes == 1 & experiment$spots == 1, 1, 0)

# Making each stimulus appear 3 times
experiment <- experiment[rep(seq_len(nrow(experiment)), each = 3), ]
```


We then have to simulate an agent using the GCM to assess the stimuli. N.B. code based on work by Sara Østergaard

First we define functions for distance and similarity and assess them. Distance takes feature values along n dimensions and calculate their euclidean distance, weighing dimensions according to pregiven weights (w). Similarity decays exponentially with distance, with a pre-given exponent (c). See plot.
```{r}
# Distance 
distance <- function(vect1, vect2, w) {
  return(sum(w * abs(vect1 - vect2)))
}

# Similarity
similarity <- function(distance, c) {
  return(exp(-c * distance))
}

# Let's assess similarity
dd <- tibble(
  expand_grid(
    distance = c(0,.1,.2, .3,.4,.5,1,1.5,2,3,4,5,6), 
    c = c(0.1, 0.2, 0.5, 0.7, 1, 1.5, 2, 3, 4, 5, 6))) %>% 
  mutate(
    similarity = similarity(distance, c)
  )

dd %>% mutate(c = factor(c)) %>%
  ggplot() +
  geom_line(aes(distance, similarity, group = c, color = c)) + 
  theme_bw()
```


With distance and similarity in place, we now need to implement an agent that can observe stimuli, put them into categories according to feedback and - once enough stimuli are collected to have at least one exemplar in each category - compare stimuli to exemplars in each category and assess which category is more likely. In other words: 1) if not enough stimuli have already been observed, the agent picks a category at random, receives feedback and adjusts accordingly the category; otherwise, 2) it assesses average distance (according to weights) from observed exemplars by category; calculates similarity (according to c); and uses the relative similarity to the two categories to produce a probability of choosing category one. then it receives feedback and adjusts category accordingly.

As usual we make the agent a function, so that we can more easily deploy it.
```{r}
### generative model ###
gcm <- function(w, c, obs, cat_one, quiet = TRUE) {
  # create an empty list to save probability of saying "1" for each trial
  r <- c()
  
  ntrials <- nrow(obs)
  
  for (i in 1:ntrials) {
    # If quiet is FALSE, print every ten trials
    if (!quiet && i %% 10 == 0) {
      print(paste("i =", i))
    }
    # if this is the first trial, or there any category with no exemplars seen yet, set the choice to random
    if (i == 1 || sum(cat_one[1:(i - 1)]) == 0 || sum(cat_one[1:(i - 1)]) == (i - 1)) {
      r <- c(r, .5)
    } else {
      similarities <- c()
      # for each previously seen stimulus assess distance and similarity
      for (e in 1:(i - 1)) {
        sim <- similarity(distance(obs[i, ], obs[e, ], w), c)
        similarities <- c(similarities, sim)
      }
      # Calculate prob of saying "1" by dividing similarity to 1 by the sum of similarity to 1 and to 2
      numerator <- 0.5 * sum(similarities[cat_one[1:(i - 1)] == 1])
      denominator <- 0.5 * sum(similarities[cat_one[1:(i - 1)] == 1]) + 0.5 * sum(similarities[cat_one[1:(i - 1)] == 0])
      r <- c(r, numerator / denominator)
    }
  }

  return(rbinom(ntrials, 1, r))
}
```

With the agent in place, we can now simulate behavior on our simulated stimuli and observe the impact of different weights and scaling parameters c.

```{r}
# function for simulation responses
simulate_responses <- function(agent, w, c) {
    
    observations <- experiment %>%
        select(c("eyes", "spots", "color", "teeth", "arms"))
    
    category <- experiment$dangerous
    
    if (w == "equal") {
        weight <- rep(1 / 5, 5)
    } else if (w == "skewed1") {
        weight <- c(0, 1)
    } else if (w == "skewed2") {
        weight <- c(0.1, 0.9)
    }

    # simulate responses
    responses <- gcm(
        weight,
        c,
        observations,
        category
    )
    
    tmp_simulated_responses <- experiment %>%
        mutate(
            trial = seq(nrow(experiment)),
            sim_response = responses,
            correct = ifelse(category == sim_response, 1, 0),
            performance = cumsum(correct) / seq_along(correct),
            c = c,
            w = w,
            agent = agent
        )

    return(tmp_simulated_responses)
}


# simulate responses
plan(multisession, workers = availableCores())

param_df <- dplyr::tibble(
    expand_grid(
        agent = 1:10,
        c = seq(.1, 2, 0.2),
        w = c("equal", "skewed1", "skewed2")
    )
)

simulated_responses <- future_pmap_dfr(param_df, 
    simulate_responses,
    .options = furrr_options(seed = TRUE)
)

write.csv(simulated_responses, "simulated_responses_10agents.csv") 
```
```{r}
simulated_responses <- read.csv("simulated_responses_10agents.csv")
```


And now we can try a few plots to better understand how the model fares and the difference made by weights and scaling factors.

```{r}
p3 <- simulated_responses %>%
  mutate(w = as.factor(w)) %>%
  ggplot(aes(trial, performance, group = w, color = w)) +
  geom_smooth() +
  theme_bw() +
  facet_wrap(c ~ .)

p4 <- simulated_responses %>%
  mutate(c = as.factor(c)) %>%
  ggplot(aes(trial, performance, group = c, color = c)) +
  geom_smooth() +
  theme_bw() +
  facet_wrap(w ~ .)

p3 + p4
```


```{r}
gcm_model <- "
// Generalized Context Model (GCM)

data {
    int<lower=1> ntrials;  // number of trials
    int<lower=1> nfeatures;  // number of predefined relevant features
    array[ntrials] int<lower=0, upper=1> cat_one; // true responses on a trial by trial basis
    array[ntrials] int<lower=0, upper=1> y;  // decisions on a trial by trial basis
    array[ntrials, nfeatures] real obs; // stimuli as vectors of features
    real<lower=0, upper=1> b;  // initial bias for category one over two

    // priors
    vector[nfeatures] w_prior_values;  // concentration parameters for dirichlet distribution <lower=1>
    array[2] real c_prior_values;  // mean and variance for logit-normal distribution
}

transformed data {
    array[ntrials] int<lower=0, upper=1> cat_two; // dummy variable for category two over cat 1
    array[sum(cat_one)] int<lower=1, upper=ntrials> cat_one_idx; // array of which stimuli are cat 1
    array[ntrials-sum(cat_one)] int<lower=1, upper=ntrials> cat_two_idx; //  array of which stimuli are cat 2
    int idx_one = 1; // Initializing 
    int idx_two = 1;
    for (i in 1:ntrials){
        cat_two[i] = abs(cat_one[i]-1);

        if (cat_one[i]==1){
            cat_one_idx[idx_one] = i;
            idx_one +=1;
        } else {
            cat_two_idx[idx_two] = i;
            idx_two += 1;
        }
    }
}

parameters {
    simplex[nfeatures] w;  // simplex means sum(w)=1
    real logit_c;
}

transformed parameters {
    // parameter c 
    real<lower=0, upper=2> c = inv_logit(logit_c)*2;  // times 2 as c is bounded between 0 and 2

    // parameter r (probability of response = category 1)
    array[ntrials] real<lower=0.0001, upper=0.9999> r;
    array[ntrials] real rr;

    for (i in 1:ntrials) {

        // calculate distance from obs to all exemplars
        array[(i-1)] real exemplar_sim;
        for (e in 1:(i-1)){
            array[nfeatures] real tmp_dist;
            for (j in 1:nfeatures) {
                tmp_dist[j] = w[j]*abs(obs[e,j] - obs[i,j]);
            }
            exemplar_sim[e] = exp(-c * sum(tmp_dist));
        }

        if (sum(cat_one[:(i-1)])==0 || sum(cat_two[:(i-1)])==0){  // if there are no examplars in one of the categories
            r[i] = 0.5;

        } else {
            // calculate similarity
            array[2] real similarities;
            
            array[sum(cat_one[:(i-1)])] int tmp_idx_one = cat_one_idx[:sum(cat_one[:(i-1)])];
            array[sum(cat_two[:(i-1)])] int tmp_idx_two = cat_two_idx[:sum(cat_two[:(i-1)])];
            similarities[1] = sum(exemplar_sim[tmp_idx_one]);
            similarities[2] = sum(exemplar_sim[tmp_idx_two]);

            // calculate r[i]
            rr[i] = (b*similarities[1]) / (b*similarities[1] + (1-b)*similarities[2]);

            // to make the sampling work
            if (rr[i] > 0.9999){
                r[i] = 0.9999;
            } else if (rr[i] < 0.0001) {
                r[i] = 0.0001;
            } else if (rr[i] > 0.0001 && rr[i] < 0.9999) {
                r[i] = rr[i];
            } else {
                r[i] = 0.5;
            }
        }
    }
}

model {
    // Priors
    target += dirichlet_lpdf(w | w_prior_values);
    target += normal_lpdf(logit_c | c_prior_values[1], c_prior_values[2]);
    
    
    // Decision Data
    target += bernoulli_lpmf(y | r);
}

generated quantities {
    // priors
    simplex[nfeatures] w_prior = dirichlet_rng(w_prior_values);
    real logit_c_prior = normal_rng(c_prior_values[1], c_prior_values[2]);
    real<lower=0, upper=2> c_prior = inv_logit(logit_c_prior)*2;

    // prior pred
    array[ntrials] real<lower=0, upper=1> r_prior;
    array[ntrials] real rr_prior;
    for (i in 1:ntrials) {

        // calculate distance from obs to all exemplars
        array[(i-1)] real exemplar_dist;
        for (e in 1:(i-1)){
            array[nfeatures] real tmp_dist;
            for (j in 1:nfeatures) {
                tmp_dist[j] = w_prior[j]*abs(obs[e,j] - obs[i,j]);
            }
            exemplar_dist[e] = sum(tmp_dist);
        }

        if (sum(cat_one[:(i-1)])==0 || sum(cat_two[:(i-1)])==0){  // if there are no examplars in one of the categories
            r_prior[i] = 0.5;

        } else {
            // calculate similarity
            array[2] real similarities;
            
            array[sum(cat_one[:(i-1)])] int tmp_idx_one = cat_one_idx[:sum(cat_one[:(i-1)])];
            array[sum(cat_two[:(i-1)])] int tmp_idx_two = cat_two_idx[:sum(cat_two[:(i-1)])];
            similarities[1] = exp(-c_prior * sum(exemplar_dist[tmp_idx_one]));
            similarities[2] = exp(-c_prior * sum(exemplar_dist[tmp_idx_two]));

            // calculate r[i]
            rr_prior[i] = (b*similarities[1]) / (b*similarities[1] + (1-b)*similarities[2]);

            // to make the sampling work
            if (rr_prior[i] == 1){
                r_prior[i] = 0.9999;
            } else if (rr_prior[i] == 0) {
                r_prior[i] = 0.0001;
            } else if (rr_prior[i] > 0 && rr_prior[i] < 1) {
                r_prior[i] = rr_prior[i];
            } else {
                r_prior[i] = 0.5;
            }
        }
    }

    array[ntrials] int<lower=0, upper=1> priorpred = bernoulli_rng(r_prior);

    // posterior pred
    array[ntrials] int<lower=0, upper=1> posteriorpred = bernoulli_rng(r);
    array[ntrials] int<lower=0, upper=1> posteriorcorrect;
    for (i in 1:ntrials) {
        if (posteriorpred[i] == cat_one[i]) {
            posteriorcorrect[i] = 1;
        } else {
            posteriorcorrect[i] = 0;
        }
    }
    
    
    // log likelihood
   array[ntrials] real log_lik;

   for (i in 1:ntrials) {
        log_lik[i] = bernoulli_lpmf(y[i] | r[i]);
   }

}"

write_stan_file(
  gcm_model,
  dir = "stan/",
  basename = "W10_GCM.stan")
```


```{r}
file <- file.path("stan/W10_GCM.stan")
mod_GCM <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))
```


Now it’s time to fit the data

```{r}
d <- simulated_responses %>% subset(
  c == "0.5" & w == "equal" & agent == 1
)

gcm_data <- list(
  ntrials = nrow(d),
  nfeatures = 5,
  cat_one = d$dangerous,
  y = d$sim_response,
  obs = as.matrix(d[, c("eyes", "spots", "color", "teeth", "arms")]),
  b = 0.5,
  w_prior_values = c(1, 1, 1, 1, 1), #uncertain prior - small certainty that they are all precisely .5
  c_prior_values = c(0, 1)  # mean and sd in stan model
)

samples_gcm <- mod_GCM$sample(
  data = gcm_data,
  seed = 123,
  chains = 1,
  parallel_chains = 1,
  threads_per_chain = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
  refresh = 500
)
```

## Model check

```{r}
samples_gcm$cmdstan_diagnose()
```

```{r}
samples_gcm$summary()
```

```{r}
draws_df <- as_draws_df(samples_gcm$draws())

cp1 <- ggplot(draws_df, aes(.iteration, c, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp2 <- ggplot(draws_df, aes(.iteration, logit_c, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp3 <- ggplot(draws_df, aes(.iteration, `w[1]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp4 <- ggplot(draws_df, aes(.iteration, `w[2]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp5 <- ggplot(draws_df, aes(.iteration, `w[3]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp6 <- ggplot(draws_df, aes(.iteration, `w[4]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp7 <- ggplot(draws_df, aes(.iteration, `w[5]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp1 + cp2 + cp3 + cp4 + cp5 + cp6 + cp7
```


```{r}
hist1 <- ggplot(draws_df) +
  geom_histogram(aes(c), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(c_prior), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = d$c[1]) +
  theme_bw()

hist2 <- ggplot(draws_df) +
  geom_histogram(aes(`w[1]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[1]`), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = 0.2) +
  theme_bw()

hist3 <- ggplot(draws_df) +
  geom_histogram(aes(`w[2]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[2]`), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = 0.2) +
  theme_bw()

hist4 <- ggplot(draws_df) +
  geom_histogram(aes(`w[3]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[3]`), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = 0.2) +
  theme_bw()

hist5 <- ggplot(draws_df) +
  geom_histogram(aes(`w[4]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[4]`), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = 0.2) +
  theme_bw()

hist6 <- ggplot(draws_df) +
  geom_histogram(aes(`w[5]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[5]`), alpha = 0.6, fill = "pink") +
  geom_vline(xintercept = 0.2) +
  theme_bw()

hist1 + hist2 + hist3 + hist4 + hist5 + hist6
```


```{r}
scat1 <- ggplot(draws_df) +
  geom_point(aes(c, `w[2]`), alpha = 0.6, color = "lightblue") +
  theme_bw()

scat2 <- ggplot(draws_df) +
  geom_point(aes(c, `w[1]`), alpha = 0.6, color = "lightblue") +
  theme_bw()

scat3 <- ggplot(draws_df) +
  geom_point(aes(`w[1]`, `w[2]`), alpha = 0.6, color = "lightblue") +
  theme_bw()

scat1 + scat2 + scat3
```




# Fitting the model on empirical data

```{r loading the data}
data <- read.csv("clean_empirical_data.csv")

# Subsetting a single participant
data <- filter(data, subject == 1)

# Calculating the performance column
data$performance = cumsum(data$correct) / seq_along(data$correct)
```


```{r}
p3 <- simulated_responses %>%
  mutate(w = as.factor(w)) %>%
  ggplot(aes(trial, performance, group = w, color = w)) +
  geom_smooth() +
  theme_bw() +
  facet_wrap(c ~ .) + 
  ggtitle("Simulated data") + 
  
  ggplot(data, aes(trial, performance)) +
  geom_smooth() +
  theme_bw() + 
  ggtitle("Empirical data")

p4 <- simulated_responses %>%
  mutate(c = as.factor(c)) %>%
  ggplot(aes(trial, performance, group = c, color = c)) +
  geom_smooth() +
  theme_bw() +
  facet_wrap(w ~ .) +
  ggtitle("Simulated data") + 
  
  ggplot(data, aes(trial, performance)) +
  geom_smooth() +
  theme_bw() + 
  ggtitle("Empirical data")

p3
p4
```


## Fitting the model
```{r}
gcm_data_empirical <- list(
  ntrials = nrow(data),
  nfeatures = 5,
  cat_one = data$dangerous,
  y = data$response,
  obs = as.matrix(data[, c("eyes", "spots", "color", "teeth", "arms")]),
  b = 0.5,
  w_prior_values = c(1, 1, 1, 1, 1), #uncertain prior - small certainty that they are all precisely .5
  c_prior_values = c(0, 1)  # mean and sd in stan model
)

samples_gcm <- mod_GCM$sample(
  data = gcm_data_empirical,
  seed = 123,
  chains = 1,
  parallel_chains = 1,
  threads_per_chain = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
  refresh = 500
)
```
## Model check

```{r}
samples_gcm$cmdstan_diagnose()
```

```{r}
samples_gcm$summary()
```


```{r}
draws_df <- as_draws_df(samples_gcm$draws())

cp1 <- ggplot(draws_df, aes(.iteration, c, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp2 <- ggplot(draws_df, aes(.iteration, logit_c, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp3 <- ggplot(draws_df, aes(.iteration, `w[1]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp4 <- ggplot(draws_df, aes(.iteration, `w[2]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp5 <- ggplot(draws_df, aes(.iteration, `w[3]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp6 <- ggplot(draws_df, aes(.iteration, `w[4]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp7 <- ggplot(draws_df, aes(.iteration, `w[5]`, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  theme_classic()

cp1 + cp2 + cp3 + cp4 + cp5 + cp6 + cp7
```


```{r}
#xxx do we not need one of these for logit_c?

hist1 <- ggplot(draws_df) +
  geom_histogram(aes(c), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(c_prior), alpha = 0.6, fill = "pink") +
  theme_bw()

hist2 <- ggplot(draws_df) +
  geom_histogram(aes(`w[1]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[1]`), alpha = 0.6, fill = "pink") +
  theme_bw()

hist3 <- ggplot(draws_df) +
  geom_histogram(aes(`w[2]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[2]`), alpha = 0.6, fill = "pink") +
  theme_bw()

hist4 <- ggplot(draws_df) +
  geom_histogram(aes(`w[3]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[3]`), alpha = 0.6, fill = "pink") +
  theme_bw()

hist5 <- ggplot(draws_df) +
  geom_histogram(aes(`w[4]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[4]`), alpha = 0.6, fill = "pink") +
  theme_bw()

hist6 <- ggplot(draws_df) +
  geom_histogram(aes(`w[5]`), alpha = 0.6, fill = "lightblue") +
  geom_histogram(aes(`w_prior[5]`), alpha = 0.6, fill = "pink") +
  theme_bw()

hist1 + hist2 + hist3 + hist4 + hist5 + hist6
```

